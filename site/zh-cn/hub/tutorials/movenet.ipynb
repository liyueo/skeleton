{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toCy3v03Dwx7"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKe-ubNcDvgv"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqtQzBCpIJ7Y"
      },
      "source": [
        "# MoveNet：超快且准确的姿态检测模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCmFOosnSkCd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://tensorflow.google.cn/hub/tutorials/movenet\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org上查看</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/hub/tutorials/movenet.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/hub/tutorials/movenet.ipynb\">     <img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">     在 GitHub 上查看源代码</a></td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/hub/tutorials/movenet.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">下载笔记本</a>   </td>\n",
        "  <td><a href=\"https://tfhub.dev/s?q=movenet\"><img src=\"https://tensorflow.google.cn/images/hub_logo_32px.png\">查看 TF Hub 模型</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x99e0aEY_d6"
      },
      "source": [
        "**[MoveNet](https://t.co/QpfnVL0YYI?amp=1)** 是一个超快且准确的模型，可检测身体的 17 个关键点。该模型在 [TF Hub](https://tfhub.dev/s?q=movenet) 上提供两种变体，分别为 Lightning 和 Thunder。Lightning 用于延迟关键型应用，而 Thunder 用于需要高准确性的应用。在大多数现代台式机、笔记本电脑和手机上，这两种模型的运行速度都快于实时 (30+ FPS)，这对于实时的健身、健康和保健应用至关重要。\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/tfjs-models/raw/master/pose-detection/assets/combined_squat_dance.gif\" alt=\"drawing\" class=\"\">\n",
        "\n",
        "*图像下载自 Pexels (https://www.pexels.com/)\n",
        "\n",
        "本 Colab 将详细介绍如何加载 MoveNet，并对下面的输入图像和视频运行推断。\n",
        "\n",
        "注：请查看[实时演示](https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=movenet)以了解该模型的工作原理！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10_zkgbZBkIE"
      },
      "source": [
        "# 使用 MoveNet 进行人体姿态估计"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u_VGR6_BmbZ"
      },
      "source": [
        "## 可视化库和导入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TtcwSIcgbIVN",
        "outputId": "02eb9e43-1991-4032-a0ed-551103e6b00e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9BLeJv-pCCld"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Import matplotlib libraries\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython.display import HTML, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bEJBMeRb3YUy"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "import cv2\n",
        "import imageio\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Dictionary that maps from joint names to keypoint indices.\n",
        "KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left_eye': 1,\n",
        "    'right_eye': 2,\n",
        "    'left_ear': 3,\n",
        "    'right_ear': 4,\n",
        "    'left_shoulder': 5,\n",
        "    'right_shoulder': 6,\n",
        "    'left_elbow': 7,\n",
        "    'right_elbow': 8,\n",
        "    'left_wrist': 9,\n",
        "    'right_wrist': 10,\n",
        "    'left_hip': 11,\n",
        "    'right_hip': 12,\n",
        "    'left_knee': 13,\n",
        "    'right_knee': 14,\n",
        "    'left_ankle': 15,\n",
        "    'right_ankle': 16\n",
        "}\n",
        "\n",
        "# Maps bones to a matplotlib color name.\n",
        "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
        "    (0, 1): 'k',  # Black\n",
        "    (0, 2): 'k',  # Black\n",
        "    (1, 3): 'k',  # Black\n",
        "    (2, 4): 'k',  # Black\n",
        "    (0, 5): 'k',  # Black\n",
        "    (0, 6): 'k',  # Black\n",
        "    (5, 7): 'k',  # Black\n",
        "    (7, 9): 'k',  # Black\n",
        "    (6, 8): 'k',  # Black\n",
        "    (8, 10): 'k', # Black\n",
        "    (5, 6): 'k',  # Black\n",
        "    (5, 11): 'k', # Black\n",
        "    (6, 12): 'k', # Black\n",
        "    (11, 12): 'k',# Black\n",
        "    (11, 13): 'k',# Black\n",
        "    (13, 15): 'k',# Black\n",
        "    (12, 14): 'k',# Black\n",
        "    (14, 16): 'k' # Black\n",
        "}\n",
        "\n",
        "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
        "                                     height,\n",
        "                                     width,\n",
        "                                     keypoint_threshold=0.11,\n",
        "                                     anomaly_level=0.0):\n",
        "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
        "\n",
        "  Args:\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    height: height of the image in pixels.\n",
        "    width: width of the image in pixels.\n",
        "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
        "      visualized.\n",
        "    anomaly_level: A float between 0 and 1 controlling the degree of anomaly.\n",
        "      Higher values result in more distorted keypoints.\n",
        "\n",
        "  Returns:\n",
        "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
        "      * the coordinates of all keypoints of all detected entities;\n",
        "      * the coordinates of all skeleton edges of all detected entities;\n",
        "      * the colors in which the edges should be plotted.\n",
        "  \"\"\"\n",
        "  keypoints_all = []\n",
        "  keypoint_edges_all = []\n",
        "  edge_colors = []\n",
        "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
        "  for idx in range(num_instances):\n",
        "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
        "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
        "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
        "    kpts_absolute_xy = np.stack(\n",
        "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
        "\n",
        "    # Add noise to keypoints based on anomaly_level\n",
        "    if anomaly_level > 0:\n",
        "      noise = np.random.normal(0, anomaly_level * 50, kpts_absolute_xy.shape)\n",
        "      kpts_absolute_xy += noise\n",
        "\n",
        "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
        "        kpts_scores > keypoint_threshold, :]\n",
        "    keypoints_all.append(kpts_above_thresh_absolute)\n",
        "\n",
        "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
        "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
        "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
        "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
        "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
        "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
        "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
        "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
        "        keypoint_edges_all.append(line_seg)\n",
        "        edge_colors.append(color)\n",
        "  if keypoints_all:\n",
        "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
        "  else:\n",
        "    keypoints_xy = np.zeros((0, 17, 2))\n",
        "\n",
        "  if keypoint_edges_all:\n",
        "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
        "  else:\n",
        "    edges_xy = np.zeros((0, 2, 2))\n",
        "  return keypoints_xy, edges_xy, edge_colors\n",
        "\n",
        "\n",
        "def draw_prediction_on_image(\n",
        "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
        "    output_image_height=None, anomaly_level=0.0):\n",
        "  \"\"\"Draws the keypoint predictions on image.\n",
        "\n",
        "  Args:\n",
        "    image: A numpy array with shape [height, width, channel] representing the\n",
        "      pixel values of the input image.\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
        "      of the crop region in normalized coordinates (see the init_crop_region\n",
        "      function below for more detail). If provided, this function will also\n",
        "      draw the bounding box on the image.\n",
        "    output_image_height: An integer indicating the height of the output image.\n",
        "      Note that the image aspect ratio will be the same as the input image.\n",
        "    anomaly_level: A float between 0 and 1 controlling the degree of anomaly.\n",
        "      Higher values result in more distorted keypoints.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array with shape [out_height, out_width, channel] representing the\n",
        "    image overlaid with keypoint predictions.\n",
        "  \"\"\"\n",
        "  height, width, channel = image.shape\n",
        "  aspect_ratio = float(width) / height\n",
        "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
        "  # To remove the huge white borders\n",
        "  fig.tight_layout(pad=0)\n",
        "  ax.margins(0)\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_xticklabels([])\n",
        "  plt.axis('off')\n",
        "\n",
        "  im = ax.imshow(image)\n",
        "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
        "  ax.add_collection(line_segments)\n",
        "  # Turn off tick labels\n",
        "  scat = ax.scatter([], [], s=60, color='k', zorder=3)  # Black keypoints\n",
        "\n",
        "  (keypoint_locs, keypoint_edges,\n",
        "   edge_colors) = _keypoints_and_edges_for_display(\n",
        "       keypoints_with_scores, height, width, anomaly_level=anomaly_level)\n",
        "\n",
        "  line_segments.set_segments(keypoint_edges)\n",
        "  line_segments.set_color(edge_colors)\n",
        "  if keypoint_edges.shape[0]:\n",
        "    line_segments.set_segments(keypoint_edges)\n",
        "    line_segments.set_color(edge_colors)\n",
        "  if keypoint_locs.shape[0]:\n",
        "    scat.set_offsets(keypoint_locs)\n",
        "\n",
        "  if crop_region is not None:\n",
        "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
        "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
        "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
        "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
        "    rect = patches.Rectangle(\n",
        "        (xmin,ymin),rec_width,rec_height,\n",
        "        linewidth=1,edgecolor='b',facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "  fig.canvas.draw()\n",
        "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "  image_from_plot = image_from_plot.reshape(\n",
        "      fig.canvas.get_width_height()[::-1] + (3,))\n",
        "  plt.close(fig)\n",
        "  if output_image_height is not None:\n",
        "    output_image_width = int(output_image_height / height * width)\n",
        "    image_from_plot = cv2.resize(\n",
        "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
        "         interpolation=cv2.INTER_CUBIC)\n",
        "  return image_from_plot\n",
        "\n",
        "def to_gif(images, duration):\n",
        "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
        "  imageio.mimsave('./animation.gif', images, duration=duration)\n",
        "  return embed.embed_file('./animation.gif')\n",
        "\n",
        "def progress(value, max=100):\n",
        "  return HTML(\"\"\"\n",
        "\n",
        "          {value}\n",
        "\n",
        "  \"\"\".format(value=value, max=max))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvrN0iQiOxhR"
      },
      "source": [
        "## 从 TF hub 加载模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zeGHgANcT7a1"
      },
      "outputs": [],
      "source": [
        "model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
        "\n",
        "if \"tflite\" in model_name:\n",
        "  if \"movenet_lightning_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  elif \"movenet_lightning_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  # Initialize the TFLite interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    # TF Lite format expects tensor type of uint8.\n",
        "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
        "    # Invoke inference.\n",
        "    interpreter.invoke()\n",
        "    # Get the model prediction.\n",
        "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return keypoints_with_scores\n",
        "\n",
        "else:\n",
        "  if \"movenet_lightning\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    model = module.signatures['serving_default']\n",
        "\n",
        "    # SavedModel format expects tensor type of int32.\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    # Run model inference.\n",
        "    outputs = model(input_image)\n",
        "    # Output is a [1, 1, 17, 3] tensor.\n",
        "    keypoints_with_scores = outputs['output_0'].numpy()\n",
        "    return keypoints_with_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h1qHYaqD9ap"
      },
      "source": [
        "## 单个图像示例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymTVR2I9x22I"
      },
      "source": [
        "本节演示对**单个图像**运行模型以预测 17 个人体关键点的最小工作示例。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I3xBq80E3N_"
      },
      "source": [
        "### 加载输入图像"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "GMO4B-wx5psP"
      },
      "outputs": [],
      "source": [
        "!curl -o input_image.jpeg https://www.shutterstock.com/shutterstock/photos/2528395917/display_1500/stock-photo-full-length-shot-of-two-male-it-developers-in-stylish-outfits-talk-business-during-walking-meeting-2528395917.jpg --silent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "lJZYQ8KYFQ6x"
      },
      "outputs": [],
      "source": [
        "# Load the input image.\n",
        "image_path = 'input_image.jpeg'\n",
        "image = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_jpeg(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_UWRdQxE6WN"
      },
      "source": [
        "### 运行推断"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "VHmTwACwFW-v",
        "outputId": "6320c02c-4099-4a14-d4ab-2a6c8a3840a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'FigureCanvasAgg' object has no attribute 'tostring_rgb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-17ba855e301d>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Visualize the predictions on the blank image with black lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0moutput_overlay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_prediction_on_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblank_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints_with_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomaly_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-755cf24e175c>\u001b[0m in \u001b[0;36mdraw_prediction_on_image\u001b[0;34m(image, keypoints_with_scores, crop_region, close_figure, output_image_height, anomaly_level)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m   \u001b[0mimage_from_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m   image_from_plot = image_from_plot.reshape(\n\u001b[1;32m    181\u001b[0m       fig.canvas.get_width_height()[::-1] + (3,))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'FigureCanvasAgg' object has no attribute 'tostring_rgb'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1636.36x1200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
        "input_image = tf.expand_dims(image, axis=0)\n",
        "input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
        "\n",
        "# Run model inference.\n",
        "keypoints_with_scores = movenet(input_image)\n",
        "\n",
        "# Create a blank white image with the same size as the original image\n",
        "blank_image = np.ones_like(image) * 255  # White background\n",
        "\n",
        "# Visualize the predictions on the blank image with black lines\n",
        "output_overlay = draw_prediction_on_image(blank_image, keypoints_with_scores, anomaly_level=0.1)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(output_overlay)\n",
        "_ = plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKm-B0eMYeg8"
      },
      "source": [
        "## 视频（图像序列）示例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdPFXabLyiKv"
      },
      "source": [
        "本节演示当输入为帧序列时，如何根据前一帧的检测结果应用智能裁剪。这样模型可以将注意力和资源投入到主要主题上，从而在不牺牲速度的情况下获得更好的预测质量。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SYFdK-JHYhrv"
      },
      "outputs": [],
      "source": [
        "#@title Cropping Algorithm\n",
        "\n",
        "# Confidence score to determine whether a keypoint prediction is reliable.\n",
        "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
        "\n",
        "def init_crop_region(image_height, image_width):\n",
        "  \"\"\"Defines the default crop region.\n",
        "\n",
        "  The function provides the initial crop region (pads the full image from both\n",
        "  sides to make it a square image) when the algorithm cannot reliably determine\n",
        "  the crop region from the previous frame.\n",
        "  \"\"\"\n",
        "  if image_width > image_height:\n",
        "    box_height = image_width / image_height\n",
        "    box_width = 1.0\n",
        "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
        "    x_min = 0.0\n",
        "  else:\n",
        "    box_height = 1.0\n",
        "    box_width = image_height / image_width\n",
        "    y_min = 0.0\n",
        "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
        "\n",
        "  return {\n",
        "    'y_min': y_min,\n",
        "    'x_min': x_min,\n",
        "    'y_max': y_min + box_height,\n",
        "    'x_max': x_min + box_width,\n",
        "    'height': box_height,\n",
        "    'width': box_width\n",
        "  }\n",
        "\n",
        "def torso_visible(keypoints):\n",
        "  \"\"\"Checks whether there are enough torso keypoints.\n",
        "\n",
        "  This function checks whether the model is confident at predicting one of the\n",
        "  shoulders/hips which is required to determine a good crop region.\n",
        "  \"\"\"\n",
        "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE) and\n",
        "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE))\n",
        "\n",
        "def determine_torso_and_body_range(\n",
        "    keypoints, target_keypoints, center_y, center_x):\n",
        "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
        "\n",
        "  The function returns the maximum distances from the two sets of keypoints:\n",
        "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
        "  used to determine the crop size. See determineCropRegion for more detail.\n",
        "  \"\"\"\n",
        "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
        "  max_torso_yrange = 0.0\n",
        "  max_torso_xrange = 0.0\n",
        "  for joint in torso_joints:\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "    if dist_y > max_torso_yrange:\n",
        "      max_torso_yrange = dist_y\n",
        "    if dist_x > max_torso_xrange:\n",
        "      max_torso_xrange = dist_x\n",
        "\n",
        "  max_body_yrange = 0.0\n",
        "  max_body_xrange = 0.0\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
        "      continue\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
        "    if dist_y > max_body_yrange:\n",
        "      max_body_yrange = dist_y\n",
        "\n",
        "    if dist_x > max_body_xrange:\n",
        "      max_body_xrange = dist_x\n",
        "\n",
        "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
        "\n",
        "def determine_crop_region(\n",
        "      keypoints, image_height,\n",
        "      image_width):\n",
        "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
        "\n",
        "  The algorithm uses the detected joints from the previous frame to estimate\n",
        "  the square region that encloses the full body of the target person and\n",
        "  centers at the midpoint of two hip joints. The crop size is determined by\n",
        "  the distances between each joints and the center point.\n",
        "  When the model is not confident with the four torso joint predictions, the\n",
        "  function returns a default crop which is the full image padded to square.\n",
        "  \"\"\"\n",
        "  target_keypoints = {}\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    target_keypoints[joint] = [\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
        "    ]\n",
        "\n",
        "  if torso_visible(keypoints):\n",
        "    center_y = (target_keypoints['left_hip'][0] +\n",
        "                target_keypoints['right_hip'][0]) / 2;\n",
        "    center_x = (target_keypoints['left_hip'][1] +\n",
        "                target_keypoints['right_hip'][1]) / 2;\n",
        "\n",
        "    (max_torso_yrange, max_torso_xrange,\n",
        "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
        "          keypoints, target_keypoints, center_y, center_x)\n",
        "\n",
        "    crop_length_half = np.amax(\n",
        "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
        "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
        "\n",
        "    tmp = np.array(\n",
        "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
        "    crop_length_half = np.amin(\n",
        "        [crop_length_half, np.amax(tmp)]);\n",
        "\n",
        "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
        "\n",
        "    if crop_length_half > max(image_width, image_height) / 2:\n",
        "      return init_crop_region(image_height, image_width)\n",
        "    else:\n",
        "      crop_length = crop_length_half * 2;\n",
        "      return {\n",
        "        'y_min': crop_corner[0] / image_height,\n",
        "        'x_min': crop_corner[1] / image_width,\n",
        "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
        "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
        "        'height': (crop_corner[0] + crop_length) / image_height -\n",
        "            crop_corner[0] / image_height,\n",
        "        'width': (crop_corner[1] + crop_length) / image_width -\n",
        "            crop_corner[1] / image_width\n",
        "      }\n",
        "  else:\n",
        "    return init_crop_region(image_height, image_width)\n",
        "\n",
        "def crop_and_resize(image, crop_region, crop_size):\n",
        "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
        "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
        "          crop_region['y_max'], crop_region['x_max']]]\n",
        "  output_image = tf.image.crop_and_resize(\n",
        "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
        "  return output_image\n",
        "\n",
        "def run_inference(movenet, image, crop_region, crop_size):\n",
        "  \"\"\"Runs model inferece on the cropped region.\n",
        "\n",
        "  The function runs the model inference on the cropped region and updates the\n",
        "  model output to the original image coordinate system.\n",
        "  \"\"\"\n",
        "  image_height, image_width, _ = image.shape\n",
        "  input_image = crop_and_resize(\n",
        "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
        "  # Run model inference.\n",
        "  keypoints_with_scores = movenet(input_image)\n",
        "  # Update the coordinates.\n",
        "  for idx in range(17):\n",
        "    keypoints_with_scores[0, 0, idx, 0] = (\n",
        "        crop_region['y_min'] * image_height +\n",
        "        crop_region['height'] * image_height *\n",
        "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
        "    keypoints_with_scores[0, 0, idx, 1] = (\n",
        "        crop_region['x_min'] * image_width +\n",
        "        crop_region['width'] * image_width *\n",
        "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
        "  return keypoints_with_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2JmA1xAEntQ"
      },
      "source": [
        "### 加载输入图像序列"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzJxbxDckWl2"
      },
      "outputs": [],
      "source": [
        "!wget -q -O dance.gif https://github.com/tensorflow/tfjs-models/raw/master/pose-detection/assets/dance_input.gif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxbMFZJUkd6W"
      },
      "outputs": [],
      "source": [
        "# Load the input image.\n",
        "image_path = 'dance.gif'\n",
        "image = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_gif(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJKeQ4siEtU9"
      },
      "source": [
        "### 使用裁剪算法运行推断"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B57XS0NZPIy"
      },
      "outputs": [],
      "source": [
        "# Load the input image.\n",
        "num_frames, image_height, image_width, _ = image.shape\n",
        "crop_region = init_crop_region(image_height, image_width)\n",
        "\n",
        "output_images = []\n",
        "bar = display(progress(0, num_frames-1), display_id=True)\n",
        "for frame_idx in range(num_frames):\n",
        "  keypoints_with_scores = run_inference(\n",
        "      movenet, image[frame_idx, :, :, :], crop_region,\n",
        "      crop_size=[input_size, input_size])\n",
        "  output_images.append(draw_prediction_on_image(\n",
        "      image[frame_idx, :, :, :].numpy().astype(np.int32),\n",
        "      keypoints_with_scores, crop_region=None,\n",
        "      close_figure=True, output_image_height=300))\n",
        "  crop_region = determine_crop_region(\n",
        "      keypoints_with_scores, image_height, image_width)\n",
        "  bar.update(progress(frame_idx, num_frames-1))\n",
        "\n",
        "# Prepare gif visualization.\n",
        "output = np.stack(output_images, axis=0)\n",
        "to_gif(output, duration=100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9u_VGR6_BmbZ",
        "5I3xBq80E3N_",
        "L2JmA1xAEntQ"
      ],
      "name": "movenet.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}