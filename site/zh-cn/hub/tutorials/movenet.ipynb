{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toCy3v03Dwx7"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKe-ubNcDvgv"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqtQzBCpIJ7Y"
      },
      "source": [
        "# MoveNet：超快且准确的姿态检测模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCmFOosnSkCd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://tensorflow.google.cn/hub/tutorials/movenet\"><img src=\"https://tensorflow.google.cn/images/tf_logo_32px.png\">在 TensorFlow.org上查看</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/zh-cn/hub/tutorials/movenet.ipynb\"><img src=\"https://tensorflow.google.cn/images/colab_logo_32px.png\">在 Google Colab 中运行</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/hub/tutorials/movenet.ipynb\">     <img src=\"https://tensorflow.google.cn/images/GitHub-Mark-32px.png\">     在 GitHub 上查看源代码</a></td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/zh-cn/hub/tutorials/movenet.ipynb\"><img src=\"https://tensorflow.google.cn/images/download_logo_32px.png\">下载笔记本</a>   </td>\n",
        "  <td><a href=\"https://tfhub.dev/s?q=movenet\"><img src=\"https://tensorflow.google.cn/images/hub_logo_32px.png\">查看 TF Hub 模型</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x99e0aEY_d6"
      },
      "source": [
        "**[MoveNet](https://t.co/QpfnVL0YYI?amp=1)** 是一个超快且准确的模型，可检测身体的 17 个关键点。该模型在 [TF Hub](https://tfhub.dev/s?q=movenet) 上提供两种变体，分别为 Lightning 和 Thunder。Lightning 用于延迟关键型应用，而 Thunder 用于需要高准确性的应用。在大多数现代台式机、笔记本电脑和手机上，这两种模型的运行速度都快于实时 (30+ FPS)，这对于实时的健身、健康和保健应用至关重要。\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/tfjs-models/raw/master/pose-detection/assets/combined_squat_dance.gif\" alt=\"drawing\" class=\"\">\n",
        "\n",
        "*图像下载自 Pexels (https://www.pexels.com/)\n",
        "\n",
        "本 Colab 将详细介绍如何加载 MoveNet，并对下面的输入图像和视频运行推断。\n",
        "\n",
        "注：请查看[实时演示](https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=movenet)以了解该模型的工作原理！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10_zkgbZBkIE"
      },
      "source": [
        "# 使用 MoveNet 进行人体姿态估计"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u_VGR6_BmbZ"
      },
      "source": [
        "## 可视化库和导入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TtcwSIcgbIVN",
        "outputId": "abb44fac-0968-43be-8884-6ecaed616899",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9BLeJv-pCCld"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Import matplotlib libraries\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython.display import HTML, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bEJBMeRb3YUy"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions for visualization\n",
        "\n",
        "# Dictionary that maps from joint names to keypoint indices.\n",
        "KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left_eye': 1,\n",
        "    'right_eye': 2,\n",
        "    'left_ear': 3,\n",
        "    'right_ear': 4,\n",
        "    'left_shoulder': 5,\n",
        "    'right_shoulder': 6,\n",
        "    'left_elbow': 7,\n",
        "    'right_elbow': 8,\n",
        "    'left_wrist': 9,\n",
        "    'right_wrist': 10,\n",
        "    'left_hip': 11,\n",
        "    'right_hip': 12,\n",
        "    'left_knee': 13,\n",
        "    'right_knee': 14,\n",
        "    'left_ankle': 15,\n",
        "    'right_ankle': 16\n",
        "}\n",
        "\n",
        "# Maps bones to a matplotlib color name.\n",
        "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
        "    (0, 1): 'k',  # Black\n",
        "    (0, 2): 'k',  # Black\n",
        "    (1, 3): 'k',  # Black\n",
        "    (2, 4): 'k',  # Black\n",
        "    (0, 5): 'k',  # Black\n",
        "    (0, 6): 'k',  # Black\n",
        "    (5, 7): 'k',  # Black\n",
        "    (7, 9): 'k',  # Black\n",
        "    (6, 8): 'k',  # Black\n",
        "    (8, 10): 'k', # Black\n",
        "    (5, 6): 'k',  # Black\n",
        "    (5, 11): 'k', # Black\n",
        "    (6, 12): 'k', # Black\n",
        "    (11, 12): 'k',# Black\n",
        "    (11, 13): 'k',# Black\n",
        "    (13, 15): 'k',# Black\n",
        "    (12, 14): 'k',# Black\n",
        "    (14, 16): 'k' # Black\n",
        "}\n",
        "\n",
        "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
        "                                     height,\n",
        "                                     width,\n",
        "                                     keypoint_threshold=0.11):\n",
        "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
        "\n",
        "  Args:\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    height: height of the image in pixels.\n",
        "    width: width of the image in pixels.\n",
        "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
        "      visualized.\n",
        "\n",
        "  Returns:\n",
        "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
        "      * the coordinates of all keypoints of all detected entities;\n",
        "      * the coordinates of all skeleton edges of all detected entities;\n",
        "      * the colors in which the edges should be plotted.\n",
        "  \"\"\"\n",
        "  keypoints_all = []\n",
        "  keypoint_edges_all = []\n",
        "  edge_colors = []\n",
        "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
        "  for idx in range(num_instances):\n",
        "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
        "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
        "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
        "    kpts_absolute_xy = np.stack(\n",
        "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
        "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
        "        kpts_scores > keypoint_threshold, :]\n",
        "    keypoints_all.append(kpts_above_thresh_absolute)\n",
        "\n",
        "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
        "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
        "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
        "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
        "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
        "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
        "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
        "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
        "        keypoint_edges_all.append(line_seg)\n",
        "        edge_colors.append(color)\n",
        "  if keypoints_all:\n",
        "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
        "  else:\n",
        "    keypoints_xy = np.zeros((0, 17, 2))\n",
        "\n",
        "  if keypoint_edges_all:\n",
        "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
        "  else:\n",
        "    edges_xy = np.zeros((0, 2, 2))\n",
        "  return keypoints_xy, edges_xy, edge_colors\n",
        "\n",
        "\n",
        "def draw_prediction_on_image(\n",
        "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
        "    output_image_height=None):\n",
        "  \"\"\"Draws the keypoint predictions on image.\n",
        "\n",
        "  Args:\n",
        "    image: A numpy array with shape [height, width, channel] representing the\n",
        "      pixel values of the input image.\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
        "      of the crop region in normalized coordinates (see the init_crop_region\n",
        "      function below for more detail). If provided, this function will also\n",
        "      draw the bounding box on the image.\n",
        "    output_image_height: An integer indicating the height of the output image.\n",
        "      Note that the image aspect ratio will be the same as the input image.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array with shape [out_height, out_width, channel] representing the\n",
        "    image overlaid with keypoint predictions.\n",
        "  \"\"\"\n",
        "  height, width, channel = image.shape\n",
        "  aspect_ratio = float(width) / height\n",
        "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
        "  # To remove the huge white borders\n",
        "  fig.tight_layout(pad=0)\n",
        "  ax.margins(0)\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_xticklabels([])\n",
        "  plt.axis('off')\n",
        "\n",
        "  im = ax.imshow(image)\n",
        "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
        "  ax.add_collection(line_segments)\n",
        "  # Turn off tick labels\n",
        "  scat = ax.scatter([], [], s=60, color='k', zorder=3)  # Black keypoints\n",
        "\n",
        "  (keypoint_locs, keypoint_edges,\n",
        "   edge_colors) = _keypoints_and_edges_for_display(\n",
        "       keypoints_with_scores, height, width)\n",
        "\n",
        "  line_segments.set_segments(keypoint_edges)\n",
        "  line_segments.set_color(edge_colors)\n",
        "  if keypoint_edges.shape[0]:\n",
        "    line_segments.set_segments(keypoint_edges)\n",
        "    line_segments.set_color(edge_colors)\n",
        "  if keypoint_locs.shape[0]:\n",
        "    scat.set_offsets(keypoint_locs)\n",
        "\n",
        "  if crop_region is not None:\n",
        "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
        "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
        "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
        "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
        "    rect = patches.Rectangle(\n",
        "        (xmin,ymin),rec_width,rec_height,\n",
        "        linewidth=1,edgecolor='b',facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "  fig.canvas.draw()\n",
        "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "  image_from_plot = image_from_plot.reshape(\n",
        "      fig.canvas.get_width_height()[::-1] + (3,))\n",
        "  plt.close(fig)\n",
        "  if output_image_height is not None:\n",
        "    output_image_width = int(output_image_height / height * width)\n",
        "    image_from_plot = cv2.resize(\n",
        "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
        "         interpolation=cv2.INTER_CUBIC)\n",
        "  return image_from_plot\n",
        "\n",
        "def to_gif(images, duration):\n",
        "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
        "  imageio.mimsave('./animation.gif', images, duration=duration)\n",
        "  return embed.embed_file('./animation.gif')\n",
        "\n",
        "def progress(value, max=100):\n",
        "  return HTML(\"\"\"\n",
        "      <progress\n",
        "          value='{value}'\n",
        "          max='{max}',\n",
        "          style='width: 100%'\n",
        "      >\n",
        "          {value}\n",
        "      </progress>\n",
        "  \"\"\".format(value=value, max=max))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvrN0iQiOxhR"
      },
      "source": [
        "## 从 TF hub 加载模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zeGHgANcT7a1"
      },
      "outputs": [],
      "source": [
        "model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
        "\n",
        "if \"tflite\" in model_name:\n",
        "  if \"movenet_lightning_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  elif \"movenet_lightning_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  # Initialize the TFLite interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    # TF Lite format expects tensor type of uint8.\n",
        "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
        "    # Invoke inference.\n",
        "    interpreter.invoke()\n",
        "    # Get the model prediction.\n",
        "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return keypoints_with_scores\n",
        "\n",
        "else:\n",
        "  if \"movenet_lightning\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    model = module.signatures['serving_default']\n",
        "\n",
        "    # SavedModel format expects tensor type of int32.\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    # Run model inference.\n",
        "    outputs = model(input_image)\n",
        "    # Output is a [1, 1, 17, 3] tensor.\n",
        "    keypoints_with_scores = outputs['output_0'].numpy()\n",
        "    return keypoints_with_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h1qHYaqD9ap"
      },
      "source": [
        "## 单个图像示例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymTVR2I9x22I"
      },
      "source": [
        "本节演示对**单个图像**运行模型以预测 17 个人体关键点的最小工作示例。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I3xBq80E3N_"
      },
      "source": [
        "### 加载输入图像"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GMO4B-wx5psP"
      },
      "outputs": [],
      "source": [
        "!curl -o input_image.jpeg https://www.shutterstock.com/shutterstock/photos/2528395917/display_1500/stock-photo-full-length-shot-of-two-male-it-developers-in-stylish-outfits-talk-business-during-walking-meeting-2528395917.jpg --silent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lJZYQ8KYFQ6x"
      },
      "outputs": [],
      "source": [
        "# Load the input image.\n",
        "image_path = 'input_image.jpeg'\n",
        "image = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_jpeg(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_UWRdQxE6WN"
      },
      "source": [
        "### 运行推断"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VHmTwACwFW-v",
        "outputId": "7257e281-9f76-43f5-fbd6-296434ce08f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'FigureCanvasAgg' object has no attribute 'tostring_rgb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-abde3ead8de5>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Visualize the predictions on the blank image with black lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0moutput_overlay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_prediction_on_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblank_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints_with_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-27352e953cab>\u001b[0m in \u001b[0;36mdraw_prediction_on_image\u001b[0;34m(image, keypoints_with_scores, crop_region, close_figure, output_image_height)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m   \u001b[0mimage_from_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   image_from_plot = image_from_plot.reshape(\n\u001b[1;32m    164\u001b[0m       fig.canvas.get_width_height()[::-1] + (3,))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'FigureCanvasAgg' object has no attribute 'tostring_rgb'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1636.36x1200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
        "input_image = tf.expand_dims(image, axis=0)\n",
        "input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
        "\n",
        "# Run model inference.\n",
        "keypoints_with_scores = movenet(input_image)\n",
        "\n",
        "# Create a blank white image with the same size as the original image\n",
        "blank_image = np.ones_like(image) * 255  # White background\n",
        "\n",
        "# Visualize the predictions on the blank image with black lines\n",
        "output_overlay = draw_prediction_on_image(blank_image, keypoints_with_scores)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(output_overlay)\n",
        "_ = plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKm-B0eMYeg8"
      },
      "source": [
        "## 视频（图像序列）示例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdPFXabLyiKv"
      },
      "source": [
        "本节演示当输入为帧序列时，如何根据前一帧的检测结果应用智能裁剪。这样模型可以将注意力和资源投入到主要主题上，从而在不牺牲速度的情况下获得更好的预测质量。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SYFdK-JHYhrv"
      },
      "outputs": [],
      "source": [
        "#@title Cropping Algorithm\n",
        "\n",
        "# Confidence score to determine whether a keypoint prediction is reliable.\n",
        "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
        "\n",
        "def init_crop_region(image_height, image_width):\n",
        "  \"\"\"Defines the default crop region.\n",
        "\n",
        "  The function provides the initial crop region (pads the full image from both\n",
        "  sides to make it a square image) when the algorithm cannot reliably determine\n",
        "  the crop region from the previous frame.\n",
        "  \"\"\"\n",
        "  if image_width > image_height:\n",
        "    box_height = image_width / image_height\n",
        "    box_width = 1.0\n",
        "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
        "    x_min = 0.0\n",
        "  else:\n",
        "    box_height = 1.0\n",
        "    box_width = image_height / image_width\n",
        "    y_min = 0.0\n",
        "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
        "\n",
        "  return {\n",
        "    'y_min': y_min,\n",
        "    'x_min': x_min,\n",
        "    'y_max': y_min + box_height,\n",
        "    'x_max': x_min + box_width,\n",
        "    'height': box_height,\n",
        "    'width': box_width\n",
        "  }\n",
        "\n",
        "def torso_visible(keypoints):\n",
        "  \"\"\"Checks whether there are enough torso keypoints.\n",
        "\n",
        "  This function checks whether the model is confident at predicting one of the\n",
        "  shoulders/hips which is required to determine a good crop region.\n",
        "  \"\"\"\n",
        "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE) and\n",
        "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE))\n",
        "\n",
        "def determine_torso_and_body_range(\n",
        "    keypoints, target_keypoints, center_y, center_x):\n",
        "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
        "\n",
        "  The function returns the maximum distances from the two sets of keypoints:\n",
        "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
        "  used to determine the crop size. See determineCropRegion for more detail.\n",
        "  \"\"\"\n",
        "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
        "  max_torso_yrange = 0.0\n",
        "  max_torso_xrange = 0.0\n",
        "  for joint in torso_joints:\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "    if dist_y > max_torso_yrange:\n",
        "      max_torso_yrange = dist_y\n",
        "    if dist_x > max_torso_xrange:\n",
        "      max_torso_xrange = dist_x\n",
        "\n",
        "  max_body_yrange = 0.0\n",
        "  max_body_xrange = 0.0\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
        "      continue\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
        "    if dist_y > max_body_yrange:\n",
        "      max_body_yrange = dist_y\n",
        "\n",
        "    if dist_x > max_body_xrange:\n",
        "      max_body_xrange = dist_x\n",
        "\n",
        "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
        "\n",
        "def determine_crop_region(\n",
        "      keypoints, image_height,\n",
        "      image_width):\n",
        "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
        "\n",
        "  The algorithm uses the detected joints from the previous frame to estimate\n",
        "  the square region that encloses the full body of the target person and\n",
        "  centers at the midpoint of two hip joints. The crop size is determined by\n",
        "  the distances between each joints and the center point.\n",
        "  When the model is not confident with the four torso joint predictions, the\n",
        "  function returns a default crop which is the full image padded to square.\n",
        "  \"\"\"\n",
        "  target_keypoints = {}\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    target_keypoints[joint] = [\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
        "    ]\n",
        "\n",
        "  if torso_visible(keypoints):\n",
        "    center_y = (target_keypoints['left_hip'][0] +\n",
        "                target_keypoints['right_hip'][0]) / 2;\n",
        "    center_x = (target_keypoints['left_hip'][1] +\n",
        "                target_keypoints['right_hip'][1]) / 2;\n",
        "\n",
        "    (max_torso_yrange, max_torso_xrange,\n",
        "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
        "          keypoints, target_keypoints, center_y, center_x)\n",
        "\n",
        "    crop_length_half = np.amax(\n",
        "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
        "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
        "\n",
        "    tmp = np.array(\n",
        "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
        "    crop_length_half = np.amin(\n",
        "        [crop_length_half, np.amax(tmp)]);\n",
        "\n",
        "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
        "\n",
        "    if crop_length_half > max(image_width, image_height) / 2:\n",
        "      return init_crop_region(image_height, image_width)\n",
        "    else:\n",
        "      crop_length = crop_length_half * 2;\n",
        "      return {\n",
        "        'y_min': crop_corner[0] / image_height,\n",
        "        'x_min': crop_corner[1] / image_width,\n",
        "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
        "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
        "        'height': (crop_corner[0] + crop_length) / image_height -\n",
        "            crop_corner[0] / image_height,\n",
        "        'width': (crop_corner[1] + crop_length) / image_width -\n",
        "            crop_corner[1] / image_width\n",
        "      }\n",
        "  else:\n",
        "    return init_crop_region(image_height, image_width)\n",
        "\n",
        "def crop_and_resize(image, crop_region, crop_size):\n",
        "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
        "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
        "          crop_region['y_max'], crop_region['x_max']]]\n",
        "  output_image = tf.image.crop_and_resize(\n",
        "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
        "  return output_image\n",
        "\n",
        "def run_inference(movenet, image, crop_region, crop_size):\n",
        "  \"\"\"Runs model inferece on the cropped region.\n",
        "\n",
        "  The function runs the model inference on the cropped region and updates the\n",
        "  model output to the original image coordinate system.\n",
        "  \"\"\"\n",
        "  image_height, image_width, _ = image.shape\n",
        "  input_image = crop_and_resize(\n",
        "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
        "  # Run model inference.\n",
        "  keypoints_with_scores = movenet(input_image)\n",
        "  # Update the coordinates.\n",
        "  for idx in range(17):\n",
        "    keypoints_with_scores[0, 0, idx, 0] = (\n",
        "        crop_region['y_min'] * image_height +\n",
        "        crop_region['height'] * image_height *\n",
        "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
        "    keypoints_with_scores[0, 0, idx, 1] = (\n",
        "        crop_region['x_min'] * image_width +\n",
        "        crop_region['width'] * image_width *\n",
        "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
        "  return keypoints_with_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2JmA1xAEntQ"
      },
      "source": [
        "### 加载输入图像序列"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzJxbxDckWl2"
      },
      "outputs": [],
      "source": [
        "!wget -q -O dance.gif https://github.com/tensorflow/tfjs-models/raw/master/pose-detection/assets/dance_input.gif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxbMFZJUkd6W"
      },
      "outputs": [],
      "source": [
        "# Load the input image.\n",
        "image_path = 'dance.gif'\n",
        "image = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_gif(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJKeQ4siEtU9"
      },
      "source": [
        "### 使用裁剪算法运行推断"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B57XS0NZPIy"
      },
      "outputs": [],
      "source": [
        "# Load the input image.\n",
        "num_frames, image_height, image_width, _ = image.shape\n",
        "crop_region = init_crop_region(image_height, image_width)\n",
        "\n",
        "output_images = []\n",
        "bar = display(progress(0, num_frames-1), display_id=True)\n",
        "for frame_idx in range(num_frames):\n",
        "  keypoints_with_scores = run_inference(\n",
        "      movenet, image[frame_idx, :, :, :], crop_region,\n",
        "      crop_size=[input_size, input_size])\n",
        "  output_images.append(draw_prediction_on_image(\n",
        "      image[frame_idx, :, :, :].numpy().astype(np.int32),\n",
        "      keypoints_with_scores, crop_region=None,\n",
        "      close_figure=True, output_image_height=300))\n",
        "  crop_region = determine_crop_region(\n",
        "      keypoints_with_scores, image_height, image_width)\n",
        "  bar.update(progress(frame_idx, num_frames-1))\n",
        "\n",
        "# Prepare gif visualization.\n",
        "output = np.stack(output_images, axis=0)\n",
        "to_gif(output, duration=100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9u_VGR6_BmbZ",
        "5I3xBq80E3N_",
        "L2JmA1xAEntQ"
      ],
      "name": "movenet.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}